{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFRgsdbfScuwWjkkGCCAR7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imanuni/imanuni/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL7zH0DhTDiC",
        "outputId": "dbcd787f-c74a-4901-b40c-6ba074951ad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from BeautifulSoup4) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install BeautifulSoup4\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import os\n"
      ],
      "metadata": {
        "id": "8Wu5HJGiTR6F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "def site_1(url):\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  section = soup.find(\"div\",class_=\"col-md-8\")\n",
        "  all_titles = section.find_all('h4', class_=\"card_title\")\n",
        "  number_of_title = len(all_titles)\n",
        "\n",
        "def site_2(url):\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  section = soup.find(\"ul\",class_=\"adjustUlLi\")\n",
        "  all_titles = section.find_all('div', class_=\"newsText ng-binding\")\n",
        "  number_of_title = len(all_titles)\n",
        "\n",
        "def site_3(url):\n",
        " response = requests.get(url)\n",
        " soup = BeautifulSoup(response.text, 'html.parser')\n",
        " title_nahar = []\n",
        " section = soup.find(\"div\", class_=\"row row--abreast\")\n",
        " all_titles = section.find_all('h3', class_=\"article__title\")\n",
        " number_of_title = len(all_titles)\n",
        "\n",
        " def main():\n",
        "  url = input(\"أدخل URL للموقع: \")\n",
        "  site_1 = \"https://www.alahednews.com.lb/catdetails.php?catid=116\"\n",
        "  site_2 = \"https://www.tayyar.org/News/Lebanon\"\n",
        "  site_3= \"https://www.annahar.com/arabic/section/1-%D9%84%D8%A8%D9%86%D8%A7%D9%86\"\n",
        "\n",
        "   if site_1_url in url:\n",
        "        all_titles, number_of_title = site_1(url)\n",
        "        title_ahad = []\n",
        "        for i in range(number_of_title):\n",
        "            title = all_titles[i].text.strip()\n",
        "            title_ahad.append({'العنوان': title})\n",
        "        print(title_ahad)\n",
        "    elif site_2_url in url:\n",
        "        all_titles, number_of_title = site_2(url)\n",
        "        title_tayar = []\n",
        "        for i in range(number_of_title):\n",
        "            title = all_titles[i].text.strip()\n",
        "            title_tayar.append({'العنوان': title})\n",
        "        print(title_tayar)\n",
        "    elif site_3_url in url:\n",
        "        all_titles, number_of_title = site_3(url)\n",
        "        title_nahar = []\n",
        "        for i in range(number_of_title):\n",
        "            title = all_titles[i].text.strip()\n",
        "            title_nahar.append({'العنوان': title})\n",
        "        print(title_nahar)\n",
        "    else:\n",
        "        print(\"الموقع غير مدعوم\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        " # Écrire l'en-tête seulement si le fichier n'existe pas déjà\n",
        "   # if not file_exists:\n",
        "  # writer.writeheader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Gc-FyazeTgqw",
        "outputId": "50d9f27b-2729-40e8-a7a2-e6edf37f0116"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 38)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    elif site_2_url in url:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import csv\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "def site_1(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    section = soup.find(\"div\", class_=\"col-md-8\")\n",
        "    all_titles = section.find_all('h4', class_=\"card_title\")\n",
        "    number_of_title = len(all_titles)\n",
        "    return all_titles, number_of_title\n",
        "\n",
        "def site_2(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    section = soup.find(\"ul\", class_=\"adjustUlLi\")\n",
        "    all_titles = section.find_all('div', class_=\"newsText ng-binding\")\n",
        "    number_of_title = len(all_titles)\n",
        "    return all_titles, number_of_title\n",
        "\n",
        "def site_3(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    section = soup.find(\"div\", class_=\"row row--abreast\")\n",
        "    all_titles = section.find_all('h3', class_=\"article__title\")\n",
        "    number_of_title = len(all_titles)\n",
        "    return all_titles, number_of_title\n",
        "\n",
        "def main():\n",
        "    url = input(\"أدخل URL للموقع: \")\n",
        "    site_1_url = \"https://www.alahednews.com.lb/catdetails.php?catid=116\"\n",
        "    site_2_url = \"https://www.tayyar.org/News/Lebanon\"\n",
        "    site_3_url = \"https://www.annahar.com/arabic/section/1-%D9%84%D8%A8%D9%86%D8%A7%D9%86\"\n",
        "\n",
        "    if site_1_url in url:\n",
        "        all_titles, number_of_title = site_1(url)\n",
        "        title_ahad = []\n",
        "        for i in range(number_of_title):\n",
        "            title = all_titles[i].text.strip()\n",
        "            title_ahad.append({'العنوان': title})\n",
        "          re\n",
        "            filename = 'iktibas.csv'\n",
        "        with open(filename, 'a', newline='',encoding='utf-8') as output_file:\n",
        "          dict_writer = csv.DictWriter(output_file, fieldnames=['العنوان'])\n",
        "          dict_writer.writeheader()\n",
        "          dict_writer.writerows(title_ahad)\n",
        "          files.download(filename)\n",
        "\n",
        "    elif site_2_url in url:\n",
        "        all_titles, number_of_title = site_2(url)\n",
        "        title_tayar = []\n",
        "        for i in range(number_of_title):\n",
        "            title = all_titles[i].text.strip()\n",
        "            title_tayar.append({'العنوان': title})\n",
        "            filename = 'iktibas.csv'\n",
        "        with open(filename, 'a', newline='',encoding='utf-8') as output_file:\n",
        "          dict_writer = csv.DictWriter(output_file, fieldnames=['العنوان'])\n",
        "          dict_writer.writeheader()\n",
        "          dict_writer.writerows(title_tayar)\n",
        "          files.download(filename)\n",
        "    elif site_3_url in url:\n",
        "        all_titles, number_of_title = site_3(url)\n",
        "        title_nahar = []\n",
        "        for i in range(number_of_title):\n",
        "            title = all_titles[i].text.strip()\n",
        "            title_nahar.append({'العنوان': title})\n",
        "            filename = 'iktibas.csv'\n",
        "        with open(filename, 'a', newline='',encoding='utf-8') as output_file:\n",
        "          dict_writer = csv.DictWriter(output_file, fieldnames=['العنوان'])\n",
        "          dict_writer.writeheader()\n",
        "          dict_writer.writerows(title_nahar)\n",
        "          files.download(filename)\n",
        "    else:\n",
        "        print(\"الموقع غير مدعوم\")\n",
        "\n",
        "\n",
        "# تنزيل الملف إلى جهازك المحلي\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "DxkmIgc5nDbw",
        "outputId": "63d40ae3-e945-4ef8-ca46-b101d99c91de"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n",
            "أدخل URL للموقع: https://www.alahednews.com.lb/catdetails.php?catid=116\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'filename' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e5b1cb735ec3>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-e5b1cb735ec3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtitle_ahad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'العنوان'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'iktibas.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m           \u001b[0mdict_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfieldnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'العنوان'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0mdict_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'filename' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import csv\n",
        "\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "def site_1(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    section = soup.find(\"div\", class_=\"row row-pad-wrap is-flex\")\n",
        "    all_titles = section.find_all('h4', class_=\"card_title\")\n",
        "    number_of_title = len(all_titles)\n",
        "    return all_titles, number_of_title\n",
        "\n",
        "def site_2(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    section = soup.find(\"ul\", class_=\"adjustUlLi\")\n",
        "    all_titles = section.find_all('div', class_=\"newsText ng-binding\")\n",
        "    number_of_title = len(all_titles)\n",
        "    return all_titles, number_of_title\n",
        "\n",
        "def site_3(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    section = soup.find(\"div\", class_=\"row row--abreast\")\n",
        "    all_titles = section.find_all('h3', class_=\"article__title\")\n",
        "    number_of_title = len(all_titles)\n",
        "    return all_titles, number_of_title\n",
        "\n",
        "def main():\n",
        "    url = input(\"أدخل URL للموقع: \")\n",
        "    site_1_url = \"https://www.alahednews.com.lb/catdetails.php?catid=113\"\n",
        "    site_2_url = \"https://www.tayyar.org/News/Lebanon\"\n",
        "    site_3_url = \"https://www.annahar.com/arabic/section/1-%D9%84%D8%A8%D9%86%D8%A7%D9%86\"\n",
        "\n",
        "    if site_1_url in url:\n",
        "        all_titles, number_of_title = site_1(url)\n",
        "        title_ahad = []\n",
        "        for i in range(number_of_title):\n",
        "            title = all_titles[i].text.strip()\n",
        "            title_ahad.append({'العنوان': title})\n",
        "        filename = 'iktibas.csv'\n",
        "        with open(filename, 'a', newline='', encoding='utf-8') as output_file:\n",
        "            dict_writer = csv.DictWriter(output_file, fieldnames=['العنوان'])\n",
        "            dict_writer.writeheader()\n",
        "            dict_writer.writerows(title_ahad)\n",
        "        print(title_ahad)\n",
        "\n",
        "    elif site_2_url in url:\n",
        "        all_titles, number_of_title = site_2(url)\n",
        "        title_tayar = []\n",
        "        for i in range(number_of_title):\n",
        "            title = all_titles[i].text.strip()\n",
        "            title_tayar.append({'العنوان': title})\n",
        "        filename = 'iktibas.csv'\n",
        "        with open(filename, 'a', newline='', encoding='utf-8') as output_file:\n",
        "            dict_writer = csv.DictWriter(output_file, fieldnames=['العنوان'])\n",
        "            dict_writer.writeheader()\n",
        "            dict_writer.writerows(title_tayar)\n",
        "        print(title_tayar)\n",
        "\n",
        "    elif site_3_url in url:\n",
        "        all_titles, number_of_title = site_3(url)\n",
        "        title_nahar = []\n",
        "        for i in range(number_of_title):\n",
        "            title = all_titles[i].text.strip()\n",
        "            title_nahar.append({'العنوان': title})\n",
        "        filename = 'iktibas.csv'\n",
        "        with open(filename, 'a', newline='', encoding='utf-8') as output_file:\n",
        "            dict_writer = csv.DictWriter(output_file, fieldnames=['العنوان'])\n",
        "            dict_writer.writeheader()\n",
        "            dict_writer.writerows(title_nahar)\n",
        "        print(title_nahar)\n",
        "\n",
        "    else:\n",
        "        print(\"الموقع غير مدعوم\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuFHhcGSC7gh",
        "outputId": "f15d1c32-03ae-4f5f-85da-a1b449b2deb1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "أدخل URL للموقع: https://www.alahednews.com.lb/catdetails.php?catid=113\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "url = f\"https://www.alahednews.com.lb/catdetails.php?catid=113\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "section = soup.find(\"div\", class_=\"col-md-6\")\n",
        "\n",
        "for all_titles in section:\n",
        "  try:\n",
        "    print(all_titles.find(class_=\"card_title\").string)\n",
        "    except:\n",
        "    continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "E0gMOoFkEfPF",
        "outputId": "befb191b-233c-479c-f4fe-54b75ec155a3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-37-e230761d6fc0>, line 12)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-e230761d6fc0>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    except:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}